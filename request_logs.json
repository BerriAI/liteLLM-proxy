{"_default": {"1": {"request_key": "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", "response_time": 1.0259308815002441, "user": "krrish@berri.ai", "created_at": 1692328106, "model": "gpt-3.5-turbo-0613", "total_cost": 6.35e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": "Clarifying the requested language: Is it \"liteLLM\" or \"Lite LLM\"?"}, "2": {"request_key": "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", "response_time": 0.909782886505127, "user": "ishaan@berri.ai", "created_at": 1692328108.1154199, "model": "command-nightly", "total_cost": 0.00045000000000000004, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": " LiteLLM is a language model that can be used to generate text based on a given prompt."}, "3": {"response_time": 0.006799936294555664, "user": "krrish@berri.ai", "created_at": 1692365843.349687, "model": "claude-instant-1", "total_cost": 1.63e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 64, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "4": {"response_time": 0.004435062408447266, "user": "krrish@berri.ai", "created_at": 1692366062.7117639, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 64, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "5": {"response_time": 0.0068759918212890625, "user": "krrish@berri.ai", "created_at": 1692366150.066541, "model": "claude-instant-1", "total_cost": 1.63e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "6": {"response_time": 0.003229856491088867, "user": "krrish@berri.ai", "created_at": 1692366181.920657, "model": "claude-instant-1", "total_cost": 1.63e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "7": {"response_time": 0.006825923919677734, "user": "krrish@berri.ai", "created_at": 1692366503.671454, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "8": {"response_time": 0.006394147872924805, "user": "krrish@berri.ai", "created_at": 1692367073.693057, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "9": {"response_time": 0.006788969039916992, "user": "krrish@berri.ai", "created_at": 1692367194.660077, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "10": {"response_time": 0.0050640106201171875, "user": "krrish@berri.ai", "created_at": 1692367267.247115, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "11": {"response_time": 0.005023002624511719, "user": "krrish@berri.ai", "created_at": 1692367311.4849532, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "12": {"response_time": 0.00492095947265625, "user": "krrish@berri.ai", "created_at": 1692367398.853602, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "13": {"response_time": 0.006744861602783203, "user": "krrish@berri.ai", "created_at": 1692367518.815507, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "14": {"response_time": 0.0064618587493896484, "user": "krrish@berri.ai", "created_at": 1692367601.004457, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "15": {"response_time": 0.007676124572753906, "user": "krrish@berri.ai", "created_at": 1692367952.4628842, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "16": {"response_time": 0.0051152706146240234, "user": "krrish@berri.ai", "created_at": 1692367998.5826821, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "17": {"request_key": "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", "response_time": 7.444961071014404, "user": "krrish@berri.ai", "created_at": 1692368006.020113, "model": "claude-instant-1", "total_cost": 0.00109828, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": " Here is a one paragraph essay in a simplified style that can be understood by most readers:\n\nLearning a new language is both difficult and rewarding. When you start learning a new language, everything seems strange and unfamiliar. The words don't sound right, the grammar is different than what you're used to, and you have trouble putting sentences together. It can feel like your brain is being asked to work in a whole new way. But if you stick with it and keep practicing everyday, little by little you'll start to understand more. Simple words and phrases will become familiar. Your pronunciation and accent will start to improve. Reading comprehension and conversation ability increases over time through consistent hard work. While learning a new language takes dedication, seeing your skills grow bit by bit is very satisfying. Mastering a new language opens up new ways of thinking and allows you to connect with more people in the world. The challenges of learning are outweighed by the many benefits of becoming multilingual."}, "18": {"response_time": 0.00702977180480957, "user": "krrish@berri.ai", "created_at": 1692368100.3468301, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "19": {"request_key": "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", "response_time": 6.530564069747925, "user": "krrish@berri.ai", "created_at": 1692368106.86482, "model": "claude-instant-1", "total_cost": 0.00102665, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": " Here is a one paragraph essay written in a more simplified language style:\n\nCaring for our world is important. The Earth gives us many good things like fresh air, clean water, plants and animals. But sometimes we hurt the Earth without meaning to. When we leave trash on the ground, it can hurt plants and animals. Trash also makes the Earth and water dirty. Many people are trying to help fix this problem. We can help too by putting our trash in the garbage can. We should also try to use less things that are bad for the Earth like plastic. If we all work together to take care of the Earth, it will stay healthy for a long time. Then future kids and their kids will get to enjoy the Earth too. They will get to see cool animals and breathe fresh air just like we do now. We need to protect the Earth so that others can enjoy it after us."}, "20": {"response_time": 0.00669097900390625, "user": "krrish@berri.ai", "created_at": 1692368168.629498, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "21": {"response_time": 0.005354881286621094, "user": "krrish@berri.ai", "created_at": 1692368219.4735959, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "22": {"request_key": "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", "response_time": 4.679172992706299, "user": "krrish@berri.ai", "created_at": 1692368224.141632, "model": "claude-instant-1", "total_cost": 0.00107624, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": " Here is a one paragraph essay written in a liteLLM style:\n\nThe other day I was chillin at home watchin some Netflix and thinking about life, you know, the usual stuff. But then I started ponderin' about how we're always on our phones these days scrollin through socials and whatnot. Don't get me wrong, I love me some Instagram and TikTok, but I was noticing how we never really be in the moment no more, you feel? Like back in the day people would just be hangin out together in person without all the distractions. So I think we should all try makin more of an effort to put the phones away sometimes and really connect with those around us. It ain't good for our mental health or our relationships if we always glued to a screen instead of each other. We needa find a better balance, and remember that real life is usually better than what we see online."}, "23": {"response_time": 0.0068359375, "user": "krrish@berri.ai", "created_at": 1692368446.20898, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "24": {"response_time": 0.006057024002075195, "user": "krrish@berri.ai", "created_at": 1692368534.359404, "model": "gpt-3.5-turbo", "total_cost": 1.5e-05, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "error": "Traceback (most recent call last):\n  File \"/Users/krrishdholakia/Documents/liteLLM-proxy/main.py\", line 65, in api_completion\n    response = completion(**data)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 243, in wrapper\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 227, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 44, in wrapper\n    result = future.result(timeout=local_timeout_duration)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/timeout.py\", line 35, in async_func\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 462, in completion\n    raise exception_type(model=model, custom_llm_provider=custom_llm_provider, original_exception=e)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 751, in exception_type\n    raise e\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/utils.py\", line 688, in exception_type\n    raise original_exception\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/litellm/main.py\", line 108, in completion\n    response = openai.ChatCompletion.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\n    ) = cls.__prepare_create_request(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 106, in __prepare_create_request\n    requestor = api_requestor.APIRequestor(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/api_requestor.py\", line 138, in __init__\n    self.api_key = key or util.default_api_key()\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/util.py\", line 186, in default_api_key\n    raise openai.error.AuthenticationError(\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"}, "25": {"request_key": "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", "response_time": 6.62284779548645, "user": "krrish@berri.ai", "created_at": 1692368540.971305, "model": "claude-instant-1", "total_cost": 0.00137929, "messages": [{"role": "user", "content": "write a 1 pg essay in liteLLM"}], "response": " Here is a one page essay in light romance prose:\n\nThe sun was beginning to set over the rolling green hills as Elaina walked along the old stone path. A light breeze stirred the towering oak trees that lined the way, sending flickering shadows dancing across the ground. It had been a long day in the village market, bartering for goods and supplies, but the solitude of the countryside path soothed her tired soul. \n\nAs she walked, her thoughts turned to Thomas. It had been nearly a week since she had seen the farmer from the neighbouring farm. His strong yet gentle manner always made her heart flutter, even from afar. Would he be finishing his chores soon and making his own way home along this very path? Elaina quickened her step slightly at the thought of a chance encounter in the gathering dusk. \n\nRounding the corner, movement in the distance caught her eye. A lone figure was walking towards her, a straw hat shielding their face from the fading light. As they drew nearer, Elaina\u2019s breath caught in her throat. It was Thomas. His worn boots scuffed the dirt path and in his hands he carried a wicker basket, no doubt filled with fresh eggs and vegetables from"}}}